# -*- coding: utf-8 -*-
"""FineTunedMaize - I.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1c1wInKZPlIqNce6RqtfTUtUs7IsvJNvU
"""

import torch
from PIL import Image
import cv2
import os
from transformers import AutoProcessor, AutoModelForCausalLM
from google.colab import files, drive
from pprint import pprint
import ipywidgets as widgets
from IPython.display import display
import io
import numpy as np
from typing import Dict, Any, Tuple
import torch.nn as nn
import gc
from datetime import datetime

drive.mount('/content/drive')

class HSVDataProcessor:
    @staticmethod
    def process_image_to_hsv(img: Image.Image) -> Dict[str, float]:
        """Process image to get HSV statistics"""
        img_array = np.array(img)
        hsv_img = cv2.cvtColor(img_array, cv2.COLOR_RGB2HSV)
        h, s, v = cv2.split(hsv_img)

        return {
            'h_mean': float(h.mean()),
            'h_std': float(h.std()),
            's_mean': float(s.mean()),
            's_std': float(s.std()),
            'v_mean': float(v.mean()),
            'v_std': float(v.std())
        }

class ModelHandler:
    def __init__(self):
        """Initialize model handler with necessary configurations"""
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.torch_dtype = torch.float32
        self.model_path = "/content/drive/My Drive/FineTuning/Trainings/Model_2701/model_state_dict.pth"
        self.categories = ['dark', 'medium', 'light']

        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.backends.cudnn.benchmark = True
            print(f"Using device: {self.device} ({torch.cuda.get_device_name(0)})")
        else:
            print(f"Using device: {self.device}")

    def load_model(self) -> Tuple[nn.Module, Any]:
        """Load the model and processor"""
        try:
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                gc.collect()

            print("Loading base model and processor...")
            model = AutoModelForCausalLM.from_pretrained(
                "microsoft/Florence-2-large",
                torch_dtype=self.torch_dtype,
                trust_remote_code=True,
                low_cpu_mem_usage=True
            )

            processor = AutoProcessor.from_pretrained(
                "microsoft/Florence-2-large",
                trust_remote_code=True
            )

            if os.path.exists(self.model_path):
                print(f"Loading saved model state from {self.model_path}")
                state_dict = torch.load(self.model_path, map_location=self.device)
                model.load_state_dict(state_dict)
                print("Model state loaded successfully")
            else:
                raise FileNotFoundError(f"No saved model found at {self.model_path}")

            model = model.to(self.device)
            model.eval()
            return model, processor

        except Exception as e:
            print(f"Error loading model: {str(e)}")
            if torch.cuda.is_available():
                print(f"GPU Memory: {torch.cuda.memory_allocated()/1024**3:.2f}GB")
            raise

def process_uploaded_image(uploaded_files: Dict, model: nn.Module, processor: Any,
                         model_handler: ModelHandler):
    """Process uploaded image and make prediction"""
    try:
        first_file_name = next(iter(uploaded_files))
        uploaded_file = uploaded_files[first_file_name]
        image = Image.open(io.BytesIO(uploaded_file['content'])).convert('RGB')
        image = image.resize((224, 224))

        hsv_processor = HSVDataProcessor()
        hsv_stats = hsv_processor.process_image_to_hsv(image)

        prompt = f"""Input Image HSV Values:
Hue: mean={hsv_stats['h_mean']:.1f}, std={hsv_stats['h_std']:.1f}
Saturation: mean={hsv_stats['s_mean']:.1f}, std={hsv_stats['s_std']:.1f}
Value: mean={hsv_stats['v_mean']:.1f}, std={hsv_stats['v_std']:.1f}
Color Category:"""

        inputs = processor(
            text=prompt,
            images=image,
            padding=True,
            truncation=True,
            return_tensors="pt"
        )

        inputs = {k: v.to(device=model_handler.device) for k, v in inputs.items()}

        if 'input_ids' in inputs:
            inputs['input_ids'] = inputs['input_ids'].long()
        inputs['decoder_input_ids'] = inputs['input_ids'].clone()

        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=50,
                num_beams=5,
                no_repeat_ngram_size=2,
                length_penalty=1.0,
                temperature=0.7,
                top_p=0.9,
                do_sample=True,
                pad_token_id=processor.tokenizer.pad_token_id,
                bos_token_id=processor.tokenizer.bos_token_id,
                eos_token_id=processor.tokenizer.eos_token_id
            )

        prediction = processor.decode(outputs[0], skip_special_tokens=True)

        try:
            category = prediction.split('Color Category:')[-1].strip()
            if not category or category in ['<s>', '</s>', '<s></s>']:
                print("\nRaw model output:", prediction)
                print("Processing HSV values for fallback classification...")

                v_mean = hsv_stats['v_mean']
                if v_mean > 220:
                    category = "dark"
                elif v_mean < 220 and v_mean > 215:
                    category = "medium"
                else:
                    category = "light"
                print(f"Fallback classification used based on V_mean: {v_mean}")

            if category not in model_handler.categories:
                print(f"Warning: Predicted category '{category}' not in expected categories")
        except Exception as e:
            print(f"Error processing prediction: {str(e)}")
            category = "Unable to determine category"

        print("\nPrediction Results:")
        print(f"HSV Statistics:")
        pprint(hsv_stats)
        print(f"\nPredicted Category: {category}")
        print(f"Confidence: Based on {'model prediction' if category in model_handler.categories else 'HSV fallback'}")

    except Exception as e:
        print(f"Error processing uploaded image: {str(e)}")
        import traceback
        traceback.print_exc()
        raise

def main():
    """Main function to run the image classification"""
    try:
        print(f"\nStarting at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} UTC")
        print("Initializing model handler...")

        model_handler = ModelHandler()
        print("Loading model and processor...")
        model, processor = model_handler.load_model()
        print("Model and processor loaded successfully")

        upload_button = widgets.FileUpload(
            accept='.jpg,.jpeg,.png',
            multiple=False,
            description='Upload Image',
            layout=widgets.Layout(width='auto')
        )

        def handle_upload(change):
            if change.new:
                print("\nProcessing uploaded image...")
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                process_uploaded_image(change.new, model, processor, model_handler)

        upload_button.observe(handle_upload, names='value')
        print("\nReady for image classification!")
        print("Upload an image to get started:")
        display(upload_button)

    except Exception as e:
        print(f"\nError in main: {str(e)}")
        if torch.cuda.is_available():
            print(f"GPU Memory: {torch.cuda.memory_allocated()/1024**3:.2f}GB")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()